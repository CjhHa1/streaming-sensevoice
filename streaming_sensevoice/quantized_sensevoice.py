import torch
import os
import json
import pickle
from pathlib import Path
from transformers import BitsAndBytesConfig
from .sensevoice import SenseVoiceSmall
from .streaming_sensevoice import StreamingSenseVoice, sensevoice_models

class QuantizedStreamingSenseVoice(StreamingSenseVoice):
    """8bité‡åŒ–ç‰ˆæœ¬çš„StreamingSenseVoice"""
    
    def __init__(self, use_8bit: bool = True, **kwargs):
        self.use_8bit = use_8bit
        self.quantization_config = None
        
        # åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°æ¥å¤„ç†å‚æ•°ä¼ é€’
        def quantized_load_model(*args, **load_kwargs):
            if len(args) >= 2:
                # ä½ç½®å‚æ•°è°ƒç”¨
                return self.load_model_quantized(args[0], args[1], use_8bit)
            elif 'model' in load_kwargs and 'device' in load_kwargs:
                # å…³é”®å­—å‚æ•°è°ƒç”¨
                return self.load_model_quantized(load_kwargs['model'], load_kwargs['device'], use_8bit)
            else:
                # æ··åˆè°ƒç”¨
                model = args[0] if len(args) > 0 else load_kwargs.get('model')
                device = args[1] if len(args) > 1 else load_kwargs.get('device')
                print(f"ğŸ”„ æ­£åœ¨åŠ è½½é‡åŒ–æ¨¡å‹: {model} on {device}")
                return self.load_model_quantized(model, device, use_8bit)
        
        # ä¿®æ”¹æ¨¡å‹åŠ è½½æ–¹å¼
        original_load_model = StreamingSenseVoice.load_model
        StreamingSenseVoice.load_model = quantized_load_model
        
        super().__init__(**kwargs)
        
        # æ¢å¤åŸå§‹æ–¹æ³•
        StreamingSenseVoice.load_model = original_load_model
    
    def load_model_quantized(self, model: str, device: str, use_8bit: bool = True) -> tuple:
        """åŠ è½½é‡åŒ–æ¨¡å‹"""
        key = f"{model}-{device}-{'8bit' if use_8bit else 'fp16'}"
        if key not in sensevoice_models:
            if use_8bit:
                # é…ç½®8bité‡åŒ–
                self.quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0,  # å¼‚å¸¸å€¼é˜ˆå€¼
                    llm_int8_skip_modules=["embed", "norm"]  # è·³è¿‡æŸäº›å±‚
                )
                
                print(f"ğŸ”„ æ­£åœ¨åŠ è½½8bité‡åŒ–æ¨¡å‹: {model}")
                model_obj, kwargs = SenseVoiceSmall.from_pretrained(
                    model=model, 
                    device=device,
                    quantization_config=self.quantization_config
                )
                print(f"âœ… 8bité‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆï¼Œå†…å­˜å ç”¨çº¦ä¸ºåŸæ¨¡å‹çš„50%")
            else:
                print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ ‡å‡†ç²¾åº¦æ¨¡å‹: {model}")
                model_obj, kwargs = SenseVoiceSmall.from_pretrained(
                    model=model, 
                    device=device
                )
                print(f"âœ… æ ‡å‡†ç²¾åº¦æ¨¡å‹åŠ è½½å®Œæˆ")
            
            model_obj = model_obj.to(device)
            model_obj.eval()
            sensevoice_models[key] = (model_obj, kwargs)
        return sensevoice_models[key]
    
    def save_quantized_model(self, save_directory: str, model_name: str = "quantized_sensevoice"):
        """
        ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
        
        Args:
            save_directory: ä¿å­˜ç›®å½•è·¯å¾„
            model_name: æ¨¡å‹åç§°
        """
        save_path = Path(save_directory)
        save_path.mkdir(parents=True, exist_ok=True)
        
        try:
            print(f"ğŸ”„ æ­£åœ¨ä¿å­˜é‡åŒ–æ¨¡å‹åˆ°: {save_path}")
            
            # 1. ä¿å­˜æ¨¡å‹ä¿¡æ¯ï¼ˆä¸ä¿å­˜é‡åŒ–æƒé‡ï¼Œå› ä¸ºä¼šè¢«åé‡åŒ–ï¼‰
            model_save_path = save_path / model_name
            model_save_path.mkdir(exist_ok=True)
            
            # å¯¹äºé‡åŒ–æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸åŒçš„ä¿å­˜ç­–ç•¥
            if self.use_8bit:
                print("â„¹ï¸ 8bité‡åŒ–æ¨¡å‹ï¼šåªä¿å­˜é‡åŒ–é…ç½®ï¼Œä¸ä¿å­˜æƒé‡")
                print("   åŸå› ï¼šbitsandbytesçš„é‡åŒ–æ˜¯åŠ¨æ€çš„ï¼Œä¿å­˜æƒé‡ä¼šè¢«åé‡åŒ–")
                print("   åŠ è½½æ—¶ä¼šé‡æ–°ä¸‹è½½æ¨¡å‹å¹¶åº”ç”¨é‡åŒ–é…ç½®")
                
                # åˆ›å»ºä¸€ä¸ªæ ‡è®°æ–‡ä»¶è¡¨ç¤ºè¿™æ˜¯é‡åŒ–æ¨¡å‹
                quantized_marker = model_save_path / "quantized_model.txt"
                with open(quantized_marker, 'w') as f:
                    f.write(f"""è¿™æ˜¯ä¸€ä¸ª8bité‡åŒ–æ¨¡å‹é…ç½®

                        é‡åŒ–ç±»å‹: bitsandbytes 8bit
                        åŸå§‹æ¨¡å‹: {self.model_path}
                        è®¾å¤‡: {self.device}

                        æ³¨æ„ï¼š
                        - æ­¤ç›®å½•ä¸åŒ…å«æ¨¡å‹æƒé‡
                        - åŠ è½½æ—¶ä¼šè‡ªåŠ¨é‡æ–°ä¸‹è½½åŸå§‹æ¨¡å‹å¹¶åº”ç”¨é‡åŒ–
                        - è¿™æ˜¯æ­£ç¡®çš„åšæ³•ï¼Œå› ä¸ºbitsandbytesé‡åŒ–æ˜¯åŠ¨æ€çš„

                        åŠ è½½æ–¹æ³•ï¼š
                        QuantizedStreamingSenseVoice.load_from_saved("{save_path}")
                        """)
                
                # ä¿å­˜å†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_info = self.get_memory_info()
                memory_file = model_save_path / "memory_info.json"
                with open(memory_file, 'w') as f:
                    json.dump(memory_info, f, indent=2, default=str)
                    
                print("âœ… é‡åŒ–æ¨¡å‹é…ç½®ä¿å­˜å®Œæˆ")
                
            else:
                # å¯¹äºéé‡åŒ–æ¨¡å‹ï¼Œä¿å­˜å®é™…æƒé‡
                print("ğŸ’¾ æ ‡å‡†ç²¾åº¦æ¨¡å‹ï¼šä¿å­˜å®Œæ•´æƒé‡")
                try:
                    # å°è¯•ä½¿ç”¨Hugging Faceæ–¹å¼ä¿å­˜
                    if hasattr(self.model, 'save_pretrained'):
                        self.model.save_pretrained(
                            model_save_path,
                            safe_serialization=True
                        )
                        print("âœ… ä½¿ç”¨ save_pretrained ä¿å­˜æ¨¡å‹")
                    else:
                        # ä½¿ç”¨PyTorchæ ‡å‡†æ–¹å¼ä¿å­˜
                        model_file = model_save_path / "pytorch_model.bin"
                        torch.save(self.model.state_dict(), model_file)
                        
                        # ä¿å­˜æ¨¡å‹é…ç½®
                        if hasattr(self.model, 'config'):
                            config_file = model_save_path / "config.json"
                            with open(config_file, 'w') as f:
                                json.dump(self.model.config.__dict__, f, indent=2)
                        
                        print("âœ… ä½¿ç”¨ PyTorch æ ‡å‡†æ–¹å¼ä¿å­˜æ¨¡å‹")
                        
                except Exception as save_error:
                    print(f"âš ï¸ æ ‡å‡†ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {save_error}")
                    # å¤‡ç”¨ä¿å­˜æ–¹æ¡ˆï¼šåªä¿å­˜æƒé‡
                    model_file = model_save_path / "model_weights.pth"
                    torch.save(self.model.state_dict(), model_file)
                    print("âœ… å·²ä¿å­˜æ¨¡å‹æƒé‡åˆ° model_weights.pth")
            
            # 2. ä¿å­˜é‡åŒ–é…ç½®
            quant_config_path = save_path / "quantization_config.json"
            if self.quantization_config:
                with open(quant_config_path, 'w') as f:
                    json.dump({
                        "quantization_type": "8bit",
                        "use_8bit": self.use_8bit,
                        "llm_int8_threshold": 6.0,
                        "llm_int8_skip_modules": ["embed", "norm"],
                        "load_in_8bit": True
                    }, f, indent=2)
            
            # 3. ä¿å­˜æ¨¡å‹å‚æ•°å’Œé…ç½®
            model_info = {
                "model_path": self.model_path,
                "device": self.device,
                "chunk_size": self.chunk_size,
                "padding": self.padding,
                "beam_size": self.beam_size,
                "contexts": self.contexts,
                "use_8bit": self.use_8bit,
                "quantization_enabled": True
            }
            
            info_path = save_path / "model_info.json"
            with open(info_path, 'w') as f:
                json.dump(model_info, f, indent=2)
            
            # 4. ä¿å­˜tokenizerç›¸å…³æ–‡ä»¶
            tokenizer_path = save_path / "tokenizer"
            tokenizer_path.mkdir(exist_ok=True)
            
            # ä¿å­˜tokenizerä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if hasattr(self, 'tokenizer') and self.tokenizer:
                tokenizer_info = {
                    "vocab_size": self.tokenizer.get_vocab_size(),
                    "bpe_model_available": hasattr(self, 'bpe_model') and self.bpe_model is not None
                }
                with open(tokenizer_path / "tokenizer_info.json", 'w') as f:
                    json.dump(tokenizer_info, f, indent=2)
            
            # 5. ä¿å­˜å‰ç«¯é…ç½®ï¼ˆFBANKç­‰ï¼‰
            frontend_info = {
                "neg_mean": self.neg_mean.tolist() if hasattr(self, 'neg_mean') else None,
                "inv_stddev": self.inv_stddev.tolist() if hasattr(self, 'inv_stddev') else None,
                "window_type": "hamming"
            }
            
            frontend_path = save_path / "frontend_config.json"
            with open(frontend_path, 'w') as f:
                json.dump(frontend_info, f, indent=2)
            
            # 6. ç”ŸæˆREADMEæ–‡ä»¶
            if self.use_8bit:
                readme_content = f"""# 8bité‡åŒ–è¯­éŸ³è¯†åˆ«æ¨¡å‹é…ç½®

## æ¨¡å‹ä¿¡æ¯
- åŸå§‹æ¨¡å‹: {self.model_path}
- é‡åŒ–ç±»å‹: 8bit (bitsandbytes)
- è¿è¡Œæ—¶å†…å­˜èŠ‚çœ: çº¦50%
- è®¾å¤‡: {self.device}

## é‡è¦è¯´æ˜
âš ï¸ **æ­¤ç›®å½•ä¸åŒ…å«æ¨¡å‹æƒé‡æ–‡ä»¶**

åŸå› ï¼š
- bitsandbytesçš„8bité‡åŒ–æ˜¯åŠ¨æ€çš„ï¼Œåœ¨æ¨ç†æ—¶è¿›è¡Œ
- ä¿å­˜çš„æƒé‡ä¼šè¢«è‡ªåŠ¨åé‡åŒ–ï¼Œå¤±å»å‹ç¼©æ•ˆæœ
- æ­£ç¡®åšæ³•æ˜¯ä¿å­˜é‡åŒ–é…ç½®ï¼ŒåŠ è½½æ—¶é‡æ–°åº”ç”¨

## æ–‡ä»¶è¯´æ˜
- `{model_name}/quantized_model.txt`: é‡åŒ–æ¨¡å‹è¯´æ˜
- `{model_name}/memory_info.json`: å†…å­˜ä½¿ç”¨ä¿¡æ¯
- `quantization_config.json`: é‡åŒ–é…ç½®å‚æ•°
- `model_info.json`: æ¨¡å‹åŸºæœ¬ä¿¡æ¯
- `frontend_config.json`: å‰ç«¯å¤„ç†é…ç½®

## ä½¿ç”¨æ–¹æ³•
```python
from streaming_sensevoice.quantized_sensevoice import QuantizedStreamingSenseVoice

# åŠ è½½é‡åŒ–æ¨¡å‹é…ç½®ï¼ˆä¼šé‡æ–°ä¸‹è½½å¹¶é‡åŒ–åŸå§‹æ¨¡å‹ï¼‰
model = QuantizedStreamingSenseVoice.load_from_saved(
    save_directory="{save_path}",
    device="cuda"  # æˆ– "cpu"
)
```

## å†…å­˜æ•ˆæœ
- ç£ç›˜å­˜å‚¨: åªä¿å­˜é…ç½®æ–‡ä»¶ï¼ˆå‡ KBï¼‰
- è¿è¡Œæ—¶å†…å­˜: æ¯”åŸæ¨¡å‹èŠ‚çœçº¦50%
- æ¨ç†é€Ÿåº¦: ä¸åŸæ¨¡å‹ç›¸è¿‘

## ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ
1. **bitsandbytesç‰¹æ€§**: é‡åŒ–æ˜¯åŠ¨æ€çš„ï¼Œä¿å­˜æƒé‡ä¼šåé‡åŒ–
2. **å®é™…éœ€æ±‚**: é‡åŒ–çš„ç›®çš„æ˜¯å‡å°‘è¿è¡Œæ—¶å†…å­˜ï¼Œä¸æ˜¯å‡å°‘å­˜å‚¨
3. **å¯é æ€§**: é‡æ–°ä¸‹è½½ç¡®ä¿æ¨¡å‹ä¸€è‡´æ€§
"""
            else:
                readme_content = f"""# æ ‡å‡†ç²¾åº¦è¯­éŸ³è¯†åˆ«æ¨¡å‹

## æ¨¡å‹ä¿¡æ¯
- åŸå§‹æ¨¡å‹: {self.model_path}
- ç²¾åº¦: æ ‡å‡†ç²¾åº¦ (float16/float32)
- è®¾å¤‡: {self.device}

## æ–‡ä»¶è¯´æ˜
- `{model_name}/`: æ¨¡å‹æƒé‡å’Œé…ç½®
- `model_info.json`: æ¨¡å‹åŸºæœ¬ä¿¡æ¯
- `frontend_config.json`: å‰ç«¯å¤„ç†é…ç½®
- `tokenizer/`: åˆ†è¯å™¨ç›¸å…³æ–‡ä»¶

## ä½¿ç”¨æ–¹æ³•
```python
from streaming_sensevoice.quantized_sensevoice import QuantizedStreamingSenseVoice

# åŠ è½½ä¿å­˜çš„æ¨¡å‹
model = QuantizedStreamingSenseVoice.load_from_saved(
    save_directory="{save_path}",
    device="cuda"  # æˆ– "cpu"
)
```

## å†…å­˜ä½¿ç”¨æƒ…å†µ
- æ¨¡å‹å¤§å°: ~{self.get_model_size_mb():.1f}MB
"""
            
            readme_path = save_path / "README.md"
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(readme_content)
            
            if self.use_8bit:
                print(f"âœ… 8bité‡åŒ–æ¨¡å‹é…ç½®ä¿å­˜å®Œæˆ!")
                print(f"ğŸ“ ä¿å­˜è·¯å¾„: {save_path.absolute()}")
                print(f"ğŸ’¾ ä¿å­˜å†…å®¹: é‡åŒ–é…ç½®ã€æ¨¡å‹ä¿¡æ¯ã€ä½¿ç”¨è¯´æ˜")
                print(f"â„¹ï¸ æ³¨æ„: æœªä¿å­˜æƒé‡æ–‡ä»¶ï¼ˆè¿™æ˜¯æ­£ç¡®çš„ï¼‰")
                print(f"ğŸ“Š è¿è¡Œæ—¶å†…å­˜èŠ‚çœ: çº¦50%")
            else:
                print(f"âœ… æ ‡å‡†ç²¾åº¦æ¨¡å‹ä¿å­˜å®Œæˆ!")
                print(f"ğŸ“ ä¿å­˜è·¯å¾„: {save_path.absolute()}")
                print(f"ğŸ’¾ åŒ…å«æ–‡ä»¶: æ¨¡å‹æƒé‡ã€é…ç½®ã€ä½¿ç”¨è¯´æ˜")
            
            return str(save_path.absolute())
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            raise e
    
    @classmethod
    def load_from_saved(cls, save_directory: str, device: str = "cuda"):
        """
        ä»ä¿å­˜çš„ç›®å½•åŠ è½½é‡åŒ–æ¨¡å‹
        
        Args:
            save_directory: ä¿å­˜ç›®å½•è·¯å¾„
            device: è®¾å¤‡ ("cuda" æˆ– "cpu")
        
        Returns:
            QuantizedStreamingSenseVoice: åŠ è½½çš„é‡åŒ–æ¨¡å‹å®ä¾‹
        """
        save_path = Path(save_directory)
        
        try:
            print(f"ğŸ”„ æ­£åœ¨ä» {save_path} åŠ è½½é‡åŒ–æ¨¡å‹...")
            
            # 1. è¯»å–æ¨¡å‹ä¿¡æ¯
            info_path = save_path / "model_info.json"
            if not info_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {info_path}")
            
            with open(info_path, 'r') as f:
                model_info = json.load(f)
            
            # 2. è¯»å–é‡åŒ–é…ç½®
            quant_config_path = save_path / "quantization_config.json"
            use_8bit = True
            if quant_config_path.exists():
                with open(quant_config_path, 'r') as f:
                    quant_config = json.load(f)
                    use_8bit = quant_config.get("use_8bit", True)
            
            # 3. åˆ›å»ºå®ä¾‹ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹è·¯å¾„é‡æ–°åŠ è½½
            # æ³¨æ„ï¼šç”±äºSenseVoiceSmallçš„ç‰¹æ®Šæ€§ï¼Œæˆ‘ä»¬é‡æ–°ä»åŸå§‹ä½ç½®åŠ è½½å¹¶åº”ç”¨é‡åŒ–
            original_model_path = model_info.get("model_path", "iic/SenseVoiceSmall")
            
            print(f"ğŸ”„ é‡æ–°åŠ è½½åŸå§‹æ¨¡å‹: {original_model_path}")
            instance = cls(
                chunk_size=model_info.get("chunk_size", 10),
                padding=model_info.get("padding", 8),
                beam_size=model_info.get("beam_size", 3),
                contexts=model_info.get("contexts"),
                device=device,
                model=original_model_path,  # ä½¿ç”¨åŸå§‹æ¨¡å‹è·¯å¾„
                use_8bit=use_8bit
            )
            
            # 4. å¦‚æœæœ‰ä¿å­˜çš„æƒé‡ï¼Œå°è¯•åŠ è½½ï¼ˆå¯é€‰ï¼‰
            saved_model_path = save_path / "quantized_sensevoice"
            if saved_model_path.exists():
                weight_files = [
                    "pytorch_model.bin",
                    "model_weights.pth"
                ]
                
                for weight_file in weight_files:
                    weight_path = saved_model_path / weight_file
                    if weight_path.exists():
                        try:
                            print(f"ğŸ”„ åŠ è½½ä¿å­˜çš„æƒé‡: {weight_file}")
                            saved_weights = torch.load(weight_path, map_location=device)
                            instance.model.load_state_dict(saved_weights)
                            print(f"âœ… æˆåŠŸåŠ è½½ä¿å­˜çš„æƒé‡")
                            break
                        except Exception as load_error:
                            print(f"âš ï¸ åŠ è½½æƒé‡å¤±è´¥: {load_error}, ä½¿ç”¨æ–°ä¸‹è½½çš„æ¨¡å‹")
                            break
            
            print(f"âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆ!")
            print(f"ğŸ“Š é‡åŒ–ç±»å‹: {'8bit' if use_8bit else 'æ ‡å‡†ç²¾åº¦'}")
            print(f"ğŸ’¾ å†…å­˜å ç”¨: çº¦ä¸ºåŸæ¨¡å‹çš„{'50%' if use_8bit else '100%'}")
            
            return instance
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise e
    
    def get_model_size_mb(self) -> float:
        """è·å–æ¨¡å‹å¤§å°ï¼ˆMBï¼‰"""
        if hasattr(self, 'model') and self.model:
            param_size = sum(p.numel() * p.element_size() for p in self.model.parameters())
            buffer_size = sum(b.numel() * b.element_size() for b in self.model.buffers())
            total_size = param_size + buffer_size
            return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
        return 0.0
    
    def get_memory_info(self) -> dict:
        """è·å–è¯¦ç»†çš„å†…å­˜ä½¿ç”¨ä¿¡æ¯"""
        if not hasattr(self, 'model') or not self.model:
            return {"error": "æ¨¡å‹æœªåŠ è½½"}
        
        info = {
            "model_size_mb": self.get_model_size_mb(),
            "quantization_enabled": self.use_8bit,
            "estimated_memory_saving": "~50%" if self.use_8bit else "0%",
            "device": self.device,
            "model_path": getattr(self, 'model_path', 'unknown')
        }
        
        # å¦‚æœåœ¨GPUä¸Šï¼Œè·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if self.device.startswith('cuda') and torch.cuda.is_available():
            info.update({
                "gpu_memory_allocated_mb": torch.cuda.memory_allocated() / (1024 * 1024),
                "gpu_memory_reserved_mb": torch.cuda.memory_reserved() / (1024 * 1024),
                "gpu_max_memory_mb": torch.cuda.max_memory_allocated() / (1024 * 1024)
            })
        
        return info