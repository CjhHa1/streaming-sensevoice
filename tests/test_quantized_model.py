#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡åŒ–æ¨¡å‹æµ‹è¯•æ–‡ä»¶
"""

import pytest
import torch
import shutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

try:
    # ä»ä¸»æ¨¡å—å¯¼å…¥
    from streaming_sensevoice import QuantizedStreamingSenseVoice
    QUANTIZATION_AVAILABLE = True
except ImportError:
    try:
        # å¦‚æœä¸»æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä»å­æ¨¡å—å¯¼å…¥
        from streaming_sensevoice.quantized_sensevoice import QuantizedStreamingSenseVoice
        QUANTIZATION_AVAILABLE = True
    except ImportError as e:
        QUANTIZATION_AVAILABLE = False
        import_error = str(e)

@pytest.mark.skipif(not QUANTIZATION_AVAILABLE, reason=f"é‡åŒ–åŠŸèƒ½ä¸å¯ç”¨: {import_error if not QUANTIZATION_AVAILABLE else ''}")
class TestQuantizedStreamingSenseVoice:
    """é‡åŒ–æ¨¡å‹æµ‹è¯•ç±»"""
    
    def test_model_creation(self):
        """æµ‹è¯•é‡åŒ–æ¨¡å‹åˆ›å»º"""
        # ä½¿ç”¨CPUé¿å…GPUä¾èµ–
        model = QuantizedStreamingSenseVoice(
            model="iic/SenseVoiceSmall",
            device="cpu",
            use_8bit=True,
            chunk_size=5,  # ä½¿ç”¨è¾ƒå°çš„chunkå‡å°‘æµ‹è¯•æ—¶é—´
            padding=2
        )
        
        assert model.use_8bit == True
        assert model.device == "cpu"
        assert model.chunk_size == 5
        assert model.padding == 2
    
    def test_memory_info(self):
        """æµ‹è¯•å†…å­˜ä¿¡æ¯è·å–"""
        model = QuantizedStreamingSenseVoice(
            model="iic/SenseVoiceSmall",
            device="cpu",
            use_8bit=True,
            chunk_size=5,
            padding=2
        )
        
        memory_info = model.get_memory_info()
        
        assert isinstance(memory_info, dict)
        assert "model_size_mb" in memory_info
        assert "quantization_enabled" in memory_info
        assert "estimated_memory_saving" in memory_info
        assert memory_info["quantization_enabled"] == True
        assert memory_info["estimated_memory_saving"] == "~50%"
    
    def test_model_save_and_load(self):
        """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        # å–å½“å‰ç›®å½•ä½œä¸ºä¸´æ—¶ç›®å½•
        from pathlib import Path
        temp_dir = Path(__file__).parent
        temp_path = Path(temp_dir)
        print(temp_path)
        # åˆ›å»ºé‡åŒ–æ¨¡å‹
        original_model = QuantizedStreamingSenseVoice(
            model="iic/SenseVoiceSmall",
            device="cpu",
            use_8bit=True,
            chunk_size=5,
            padding=2
        )
        
        # ä¿å­˜æ¨¡å‹
        save_path = original_model.save_quantized_model(
            save_directory=str(temp_path / "saved_model"),
            model_name="test_quantized"
        )
        
        assert Path(save_path).exists()
        
        # æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶
        saved_files = list(Path(save_path).rglob("*"))
        expected_files = [
            "model_info.json",
            "quantization_config.json", 
            "frontend_config.json",
            "README.md"
        ]
        
        saved_file_names = [f.name for f in saved_files if f.is_file()]
        for expected_file in expected_files:
            assert expected_file in saved_file_names, f"ç¼ºå°‘æ–‡ä»¶: {expected_file}"
        
        # åŠ è½½æ¨¡å‹
        loaded_model = QuantizedStreamingSenseVoice.load_from_saved(
            save_directory=save_path,
            device="cpu"
        )
        
        # éªŒè¯åŠ è½½çš„æ¨¡å‹å±æ€§
        assert loaded_model.use_8bit == original_model.use_8bit
        assert loaded_model.device == "cpu"
    
    def test_standard_vs_quantized(self):
        """æµ‹è¯•æ ‡å‡†æ¨¡å‹ä¸é‡åŒ–æ¨¡å‹çš„å¯¹æ¯”"""
        # æ ‡å‡†æ¨¡å‹
        standard_model = QuantizedStreamingSenseVoice(
            model="iic/SenseVoiceSmall",
            device="cpu",
            use_8bit=False,
            chunk_size=5,
            padding=2
        )
        
        # é‡åŒ–æ¨¡å‹
        quantized_model = QuantizedStreamingSenseVoice(
            model="iic/SenseVoiceSmall",
            device="cpu", 
            use_8bit=True,
            chunk_size=5,
            padding=2
        )
        
        standard_info = standard_model.get_memory_info()
        quantized_info = quantized_model.get_memory_info()
        
        # éªŒè¯é‡åŒ–æ¨¡å‹ç¡®å®å¯ç”¨äº†é‡åŒ–
        assert standard_info["quantization_enabled"] == False
        assert quantized_info["quantization_enabled"] == True
        
        # éªŒè¯å†…å­˜èŠ‚çœä¼°ç®—
        assert quantized_info["estimated_memory_saving"] == "~50%"
        assert standard_info["estimated_memory_saving"] == "0%"

def test_quantization_import():
    """æµ‹è¯•é‡åŒ–ç›¸å…³åŒ…çš„å¯¼å…¥"""
    try:
        import torch
        import transformers
        from transformers import BitsAndBytesConfig
        assert True, "æ‰€æœ‰å¿…éœ€çš„åŒ…éƒ½å¯ä»¥å¯¼å…¥"
    except ImportError as e:
        pytest.skip(f"ç¼ºå°‘å¿…éœ€çš„åŒ…: {e}")

if __name__ == "__main__":
    # ç®€å•çš„åŠŸèƒ½æµ‹è¯•
    print("ğŸ§ª è¿è¡Œé‡åŒ–æ¨¡å‹åŸºç¡€æµ‹è¯•...")
    
    if QUANTIZATION_AVAILABLE:
        print("âœ… é‡åŒ–åŠŸèƒ½å¯ç”¨")
        
        try:
            # åŸºç¡€æµ‹è¯•
            TestQuantizedStreamingSenseVoice().test_model_save_and_load()
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    else:
        print(f"âŒ é‡åŒ–åŠŸèƒ½ä¸å¯ç”¨: {import_error}")